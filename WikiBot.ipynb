{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Wiki Game Bot</h2><br>\n",
    "This notebook steps through a program designed to find an optimal solution for <a href='https://en.wikipedia.org/wiki/Wikipedia:Wiki_Game'>The Wiki Game</a>, where players compete to find the shortest path between 2 random wikipedia articles by clicking links within the page. \n",
    "Our strategy is to use <a href='https://nlp.stanford.edu/projects/glove/'>GloVe</a> word vectors to pick the links that match the target link the best. To measure similarity between links we are using the cosine similarity formula.\n",
    "<br>\n",
    "\\begin{equation}\n",
    "\\cos ({\\bf a},{\\bf b})= {{\\bf a} {\\bf b} \\over \\|{\\bf a}\\| \\|{\\bf b}\\|} = \\frac{ \\sum_{i=1}^{n}{{\\bf a}_i{\\bf b}_i} }{ \\sqrt{\\sum_{i=1}^{n}{({\\bf a}_i)^2}} \\sqrt{\\sum_{i=1}^{n}{({\\bf b}_i)^2}} }\n",
    "\\end{equation}\n",
    "<br>\n",
    "The perfect solution could be computed by checking every link on every page for the target link, however this is exponentially computationally expensive. To find a balance between computing speed and the depth of search we are using a proccess inspired by <a href='http://bradley.bradley.edu/~chris/searches.html#bs'>Beam Search</a>. By only searching the top n links (with the highest cosine similarity) the program is generally able to find a solution quickly. The program has certain limitations due to its use of the word embeddings. If the words of an article name aren't in the embedding dictionary then it's unable to converge on the target. If an article is a person, the embeddings are limited with names and may only converge on celebrities and famous names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Znd53IBytrdJ"
   },
   "outputs": [],
   "source": [
    "def create_embeddings():\n",
    "    global embeddings_index\n",
    "    embeddings_index = {}\n",
    "\n",
    "    #Open the 300 dimensional embedding as a readable file \n",
    "    with open('glove.6B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "        #Loop Through Each Line\n",
    "        for line in f:\n",
    "            #Lower & Strip the line, then split into a list\n",
    "            glove_parsing = line.lower().strip().split()\n",
    "\n",
    "            #Take the first element (which is a word)\n",
    "            embedding_word = glove_parsing[0]\n",
    "\n",
    "            #Then take the embedding weights and place them into an array\n",
    "            embedding_weights = np.array(glove_parsing[1:])\n",
    "\n",
    "            #Create a dictionary where keys correspond to a given word and the values are the given weights  \n",
    "            embeddings_index[embedding_word] = embedding_weights\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MRR6Rf73vy_3"
   },
   "outputs": [],
   "source": [
    "def retrive_wiki_links(url):\n",
    "    \"\"\"Given a string input, url, returns a list of all blue article links\"\"\"\n",
    "    \n",
    "    #Retrive the Entire html of the Wiki Page and Convert to a String from Bytes  \n",
    "    html = str(urlopen(url).read())\n",
    "\n",
    "    #Find the Start of the Content Page on the html\n",
    "    index_start = html.index('id=\"mw-content-text\"')\n",
    "    \n",
    "    #Find Where the End of the Content Page is, this is Usually Where the Reference Header Begins\n",
    "    #By Doing this we also Avoid Picking up the Links in the Reference Section \n",
    "    index_end = None\n",
    "    if ('id=\"References\"') in html:\n",
    "        index_end = html.index('id=\"References\"')\n",
    "    #Sometimes a Note Secection Exists, Which Comes Before the Reference Section, which also Contains Links we Wish to Avoid\n",
    "    if ('id=\"Notes\"') in html:\n",
    "        index_end = html.index('id=\"Notes\"')\n",
    "\n",
    "    #Concatenate the html to only include the Content Section \n",
    "    html_clean = html[:index_start+1] + html[:index_end]\n",
    "\n",
    "    #Process the html data using the Beautiful Soup library \n",
    "    cleaned_data = BeautifulSoup(html_clean, 'html.parser')\n",
    "\n",
    "    #Retrive a list of all anchor tags \n",
    "    tags = cleaned_data('a')\n",
    "\n",
    "    #Retrive href links \n",
    "    tag_list = []\n",
    "    for tag in tags:\n",
    "        tag = tag.get('href')\n",
    "        tag_list.append(tag)\n",
    "\n",
    "    #Remvoe None type elements\n",
    "    tag_list = [x for x in tag_list if x != None]\n",
    "\n",
    "    #Loop through the tags and filter\n",
    "    wiki_links = []\n",
    "    for tag in tag_list:\n",
    "        #Blue links have no unique identifiers compared to other links, therefore other link types must be removed\n",
    "        removed_terms = ['.jpg', '.jpeg', '/User:', '/wiki/Talk', '.png', '/wiki/Wikipedia',\n",
    "                         '/wiki/Help', '/wiki/File', '/wiki/Template', '/wiki/Special', 'https://',\n",
    "                         'disambiguation']\n",
    "        if ('/wiki/') in tag and not any(substring in tag for substring in removed_terms):\n",
    "            wiki_links.append(tag)\n",
    "    return wiki_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oT2QmC7SU-0m"
   },
   "outputs": [],
   "source": [
    "def wiki_link_cleaner(link):\n",
    "    \"\"\"Clean the raw link into words readable by the encodings\"\"\"\n",
    "    \n",
    "    link = re.sub('^(.*?)\\/wiki\\/', '', link) \n",
    "    link = link.replace('_', ' ').split('#')[0]\n",
    "    link = re.sub(r'[^\\w\\s]',' ', link)\n",
    "    return link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DOi6fJsk8LBZ"
   },
   "outputs": [],
   "source": [
    "def words_to_embeddings(wiki_links):\n",
    "    \"\"\"given a list of links(wiki_links), return them as the embedding of the words\"\"\"\n",
    "    \n",
    "    #Strip the links down to the keywords\n",
    "    clean_links = []\n",
    "    for link in wiki_links:\n",
    "        link = wiki_link_cleaner(link)\n",
    "        clean_links.append(link)\n",
    "    \n",
    "    #Loop through the links to make embeddings\n",
    "    embedding_tags = []\n",
    "    for link in clean_links:\n",
    "        words = link.split()\n",
    "\n",
    "        #If the word isn't within the embeddings, it is removed\n",
    "        for word in words:\n",
    "            if embeddings_index.get(word.lower()) is None:\n",
    "                words.remove(word)\n",
    "    \n",
    "        #Create a 2d array (length of words by the dimesnion of the word embedding) \n",
    "        word_vecs = np.zeros((len(words), 300))\n",
    "\n",
    "        #Add the respective weights to the array\n",
    "        for i in range(len(words)):\n",
    "            word_vecs[i] =  embeddings_index.get(words[i].lower())\n",
    "\n",
    "        #Take the mean of the weights (used for multi word links)\n",
    "        #It's possible to use a more complex method of multi-word embeddings\n",
    "        embedding_tags.append(np.mean(word_vecs, axis=0))\n",
    "\n",
    "    embedding_tags = np.array(embedding_tags)\n",
    "    return embedding_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rsDIH6US0j42"
   },
   "outputs": [],
   "source": [
    "#Function used to determine the similarity between words \n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calulates the cosine similarity between vectors a and b\"\"\"\n",
    "    \n",
    "    numerator = np.dot(a, b)\n",
    "    denominator = np.dot(np.linalg.norm(a), np.linalg.norm(b))\n",
    "    return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u796cpzJ2aE-"
   },
   "outputs": [],
   "source": [
    "\n",
    "def beam_search(target, embedding_tags, BEAM_VALUE):\n",
    "    \"\"\"Uses the principles of a beam search algorithm to find the best path to a range of new links\"\"\"\n",
    "    \n",
    "    tag_values = []\n",
    "    target = target.lower()\n",
    "    words = target.split()\n",
    "\n",
    "    #If the word isn't within the embeddings, it is removed\n",
    "    for word in words:\n",
    "        if embeddings_index.get(word) is None:\n",
    "            words.remove(word)\n",
    "  \n",
    "    #Create a 2d array (length of words by the dimesnion of the word embedding) \n",
    "    word_vecs = np.zeros((len(words), 300))\n",
    "\n",
    "    #Add the respective weights to the array\n",
    "    for i in range(len(words)):\n",
    "        word_vecs[i] =  embeddings_index.get(words[i])\n",
    "\n",
    "    #Take the mean of the weights (used for multi word links)\n",
    "    target_embedding = np.mean(word_vecs, axis=0)\n",
    "\n",
    "    #Calculate the cosine similarities\n",
    "    for i in range(len(embedding_tags)):\n",
    "        tag_values.append(cosine_similarity(embedding_tags[i], target_embedding))\n",
    "\n",
    "    for j in range(len(tag_values)):\n",
    "        if np.isnan(tag_values[j]):\n",
    "            tag_values[j] = 0\n",
    "\n",
    "    #Beam value adjusted if is larger than the number of values\n",
    "    temp = 0\n",
    "    if len(tag_values) < BEAM_VALUE:\n",
    "        temp = BEAM_VALUE\n",
    "        BEAM_VALUE = len(tag_values)\n",
    "\n",
    "    #Output the top BEAM_VALUE number of links\n",
    "    output = np.argpartition(np.array(tag_values), -BEAM_VALUE)[-BEAM_VALUE:]\n",
    "\n",
    "    BEAM_VALUE = temp\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kcgx14_Tae9S"
   },
   "outputs": [],
   "source": [
    "def dirty_to_searchable(index, word_list, return_link = False):\n",
    "    \"\"\"takes a half link and makes it a full wikipedia link\"\"\"\n",
    "    \n",
    "    index = int(index)\n",
    "    link = word_list[index]\n",
    "    if return_link == True:\n",
    "        link = 'https://en.wikipedia.org'+link\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1lu76fAntX-2"
   },
   "outputs": [],
   "source": [
    "def searchable_to_dirty(link):\n",
    "    \"\"\"removes the wikipedia part of the link\"\"\"\n",
    "    return link.split(\".wikipedia.org\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "In5fMVx8fKuG"
   },
   "outputs": [],
   "source": [
    "def wiki_game(starting_link, target_link):\n",
    "    target = wiki_link_cleaner(target_link)\n",
    "    blacklist = []\n",
    "    link_position = {}\n",
    "    number_of_searches = 0\n",
    "    searches = []\n",
    "    searches_history = []\n",
    "    beam = 8\n",
    "\n",
    "    #initial page search\n",
    "    wiki_links = retrive_wiki_links(starting_link)\n",
    "\n",
    "    #Create embeddings of the links\n",
    "    embedding_tags = words_to_embeddings(wiki_links)\n",
    "    \n",
    "    #Run beam search to find the best embeddings (best 12 links on the first page)\n",
    "    beam_initial_positions = beam_search(target, embedding_tags, 12)\n",
    "\n",
    "    #Format new links from the beam search\n",
    "    for i in beam_initial_positions:\n",
    "        searches.append(dirty_to_searchable(i, wiki_links, True))\n",
    "    searches_history.append(searches)\n",
    "    \n",
    "    #Keep checking new links until the target_link is found\n",
    "    print('This may take a minute...')\n",
    "    while target_link not in searches:\n",
    "        positions_beam_1 = []\n",
    "        total_links_beam_1 = []\n",
    "        total_links_beam_2 = []\n",
    "        number_of_searches += 1 \n",
    "        \n",
    "        #Limit the number of searches to prevent an infinite loop\n",
    "        if number_of_searches > 9:\n",
    "            print('Max search limit exceeded')\n",
    "            is_successful = False\n",
    "            break\n",
    "        \n",
    "        #Filter out links that have already been used to prevent looping\n",
    "        for i in searches:\n",
    "            wiki_links = retrive_wiki_links(i)\n",
    "            for wiki_link in wiki_links:\n",
    "                while wiki_links.count(wiki_link) != 1:\n",
    "                    wiki_links.remove(wiki_link)\n",
    "                if wiki_link in blacklist:\n",
    "                    wiki_links.remove(wiki_link)\n",
    "                else:\n",
    "                    total_links_beam_1.append(wiki_link)\n",
    "                    blacklist.append(wiki_link)\n",
    "\n",
    "        for x in total_links_beam_1:\n",
    "            while total_links_beam_1.count(x) != 1:\n",
    "                total_links_beam_1.remove(x)\n",
    "\n",
    "        #Beam search starts by picking top 8 links, and increases its beam by 3 each iteration\n",
    "        embedding_tags = words_to_embeddings(total_links_beam_1)\n",
    "        positions_beam_1 = beam_search(target, embedding_tags, beam)\n",
    "        beam += 3\n",
    "        \n",
    "        #format the new links and add them to searches_history for the next iteration\n",
    "        searches = []\n",
    "        for position in positions_beam_1:\n",
    "            link = dirty_to_searchable(position, total_links_beam_1, True)\n",
    "            searches.append(link) \n",
    "        searches_history.append(searches)\n",
    "        print('searching...')\n",
    "        \n",
    "    return searches_history, starting_link, target_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wE565hEjZsuI"
   },
   "outputs": [],
   "source": [
    "\n",
    "def track_link_path(searches_history, starting_link, target_link):\n",
    "    \"\"\"Works backwards from the solution, using searches_history to find a path to the starting_link\"\"\"\n",
    "    \n",
    "    path = []\n",
    "    term = searchable_to_dirty(target_link)\n",
    "    path.append(target_link)\n",
    "    for j in range(len(searches_history))[:-1][::-1]:\n",
    "        for i in searches_history[j]:\n",
    "            wiki_links = retrive_wiki_links(i)\n",
    "            for link in wiki_links:\n",
    "                if link == term:\n",
    "                    path.append(i)\n",
    "                    term = searchable_to_dirty(i)\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "    path.append(starting_link)\n",
    "    return path[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = create_embeddings() #Takes a minute to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D4V1AzOJaiL-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Link --- https://en.wikipedia.org/wiki/Apple_pie\n",
      "Target Link --- https://en.wikipedia.org/wiki/Shrek\n",
      "This may take a minute...\n",
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "https://en.wikipedia.org/wiki/Apple_pie ---> https://en.wikipedia.org/wiki/Ginger ---> https://en.wikipedia.org/wiki/Gingerbread ---> https://en.wikipedia.org/wiki/Gingerbread_man ---> https://en.wikipedia.org/wiki/Shrek\n"
     ]
    }
   ],
   "source": [
    "starting_link = input('Starting Link --- ')\n",
    "target_link = input('Target Link --- ')\n",
    "\n",
    "searches_history, starting_link, target_link = wiki_game(starting_link, target_link)\n",
    "path = track_link_path(searches_history, starting_link, target_link)\n",
    "\n",
    "for i in range(len(path)-1):\n",
    "    print(path[i], end = \" ---> \")\n",
    "print(path[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This may take a minute...\n",
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "https://en.wikipedia.org/wiki/Interference_(communication) ---> https://en.wikipedia.org/wiki/Wireless_networks ---> https://en.wikipedia.org/wiki/Wi-Fi ---> https://en.wikipedia.org/wiki/Microwave_oven ---> https://en.wikipedia.org/wiki/Popcorn\n"
     ]
    }
   ],
   "source": [
    "starting_link = 'https://en.wikipedia.org/wiki/Interference_(communication)'\n",
    "target_link = 'https://en.wikipedia.org/wiki/Popcorn'\n",
    "\n",
    "searches_history, starting_link, target_link = wiki_game(starting_link, target_link)\n",
    "path = track_link_path(searches_history, starting_link, target_link)\n",
    "\n",
    "for i in range(len(path)-1):\n",
    "    print(path[i], end = \" ---> \")\n",
    "print(path[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This may take a minute...\n",
      "searching...\n",
      "searching...\n",
      "https://en.wikipedia.org/wiki/Early_Christianity ---> https://en.wikipedia.org/wiki/Christian_music ---> https://en.wikipedia.org/wiki/Music_download ---> https://en.wikipedia.org/wiki/Eminem\n"
     ]
    }
   ],
   "source": [
    "starting_link = 'https://en.wikipedia.org/wiki/Early_Christianity'\n",
    "target_link = 'https://en.wikipedia.org/wiki/Eminem'\n",
    "\n",
    "searches_history, starting_link, target_link = wiki_game(starting_link, target_link)\n",
    "path = track_link_path(searches_history, starting_link, target_link)\n",
    "\n",
    "for i in range(len(path)-1):\n",
    "    print(path[i], end = \" ---> \")\n",
    "print(path[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This may take a minute...\n",
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "https://en.wikipedia.org/wiki/Apple_pie ---> https://en.wikipedia.org/wiki/Ginger ---> https://en.wikipedia.org/wiki/Gingerbread ---> https://en.wikipedia.org/wiki/Gingerbread_man ---> https://en.wikipedia.org/wiki/Shrek\n"
     ]
    }
   ],
   "source": [
    "starting_link = 'https://en.wikipedia.org/wiki/Apple_pie'\n",
    "target_link = 'https://en.wikipedia.org/wiki/Shrek'\n",
    "\n",
    "searches_history, starting_link, target_link = wiki_game(starting_link, target_link)\n",
    "path = track_link_path(searches_history, starting_link, target_link)\n",
    "\n",
    "for i in range(len(path)-1):\n",
    "    print(path[i], end = \" ---> \")\n",
    "print(path[-1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Beam.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
